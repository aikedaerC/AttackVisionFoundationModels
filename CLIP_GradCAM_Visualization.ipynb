{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aikedaerC/AttackVisionFoundationModels/blob/main/CLIP_GradCAM_Visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpPbEKLBmyX1"
      },
      "source": [
        "# CLIP GradCAM Colab\n",
        "\n",
        "This Colab notebook uses [GradCAM](https://arxiv.org/abs/1610.02391) on OpenAI's [CLIP](https://openai.com/blog/clip/) model to produce a heatmap highlighting which regions in an image activate the most to a given caption.\n",
        "\n",
        "**Note:** Currently only works with the ResNet variants of CLIP. ViT support coming soon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qQOvOhnKQ-Tu"
      },
      "outputs": [],
      "source": [
        "#@title Install dependencies\n",
        "\n",
        "#@markdown Please execute this cell by pressing the _Play_ button\n",
        "#@markdown on the left.\n",
        "\n",
        "#@markdown **Note**: This installs the software on the Colab\n",
        "#@markdown notebook in the cloud and not on your computer.\n",
        "\n",
        "%%capture\n",
        "!pip install ftfy regex tqdm matplotlib opencv-python scipy scikit-image\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import clip\n",
        "from PIL import Image\n",
        "from scipy.ndimage import filters\n",
        "from torch import nn\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "caPbAhFlRBwT"
      },
      "outputs": [],
      "source": [
        "#@title Helper functions\n",
        "\n",
        "#@markdown Some helper functions for overlaying heatmaps on top\n",
        "#@markdown of images and visualizing with matplotlib.\n",
        "\n",
        "def normalize(x: np.ndarray) -> np.ndarray:\n",
        "    # Normalize to [0, 1].\n",
        "    x = x - x.min()\n",
        "    if x.max() > 0:\n",
        "        x = x / x.max()\n",
        "    return x\n",
        "\n",
        "# Modified from: https://github.com/salesforce/ALBEF/blob/main/visualization.ipynb\n",
        "def getAttMap(img, attn_map, blur=True):\n",
        "    if blur:\n",
        "        attn_map = filters.gaussian_filter(attn_map, 0.02*max(img.shape[:2]))\n",
        "    attn_map = normalize(attn_map)\n",
        "    cmap = plt.get_cmap('jet')\n",
        "    attn_map_c = np.delete(cmap(attn_map), 3, 2)\n",
        "    attn_map = 1*(1-attn_map**0.7).reshape(attn_map.shape + (1,))*img + \\\n",
        "            (attn_map**0.7).reshape(attn_map.shape+(1,)) * attn_map_c\n",
        "    return attn_map\n",
        "\n",
        "def viz_attn(img, attn_map, blur=True):\n",
        "    _, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(img)\n",
        "    axes[1].imshow(getAttMap(img, attn_map, blur))\n",
        "    for ax in axes:\n",
        "        ax.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "def viz_attn(ori, img, attn_map, blur=True):\n",
        "    # Ensure the attention map is resized to match the original image dimensions\n",
        "    attn_map_resized = cv2.resize(attn_map, (ori.shape[0], ori.shape[1]))\n",
        "\n",
        "    # Visualize the original image and attention map overlay\n",
        "    _, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(ori)\n",
        "    axes[0].set_title(\"Original Image\")\n",
        "\n",
        "    # Pass the resized attention map to getAttMap\n",
        "    axes[1].imshow(getAttMap(ori, attn_map_resized, blur))\n",
        "    axes[1].set_title(\"Attention Overlay\")\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "def load_image(img_path, resize=None):\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    if resize is not None:\n",
        "        image = image.resize((resize, resize))\n",
        "    return np.asarray(image).astype(np.float32) / 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "XziodsCqVC2A"
      },
      "outputs": [],
      "source": [
        "#@title GradCAM: Gradient-weighted Class Activation Mapping\n",
        "\n",
        "#@markdown Our gradCAM implementation registers a forward hook\n",
        "#@markdown on the model at the specified layer. This allows us\n",
        "#@markdown to save the intermediate activations and gradients\n",
        "#@markdown at that layer.\n",
        "\n",
        "#@markdown To visualize which parts of the image activate for\n",
        "#@markdown a given caption, we use the caption as the target\n",
        "#@markdown label and backprop through the network using the\n",
        "#@markdown image as the input.\n",
        "#@markdown In the case of CLIP models with resnet encoders,\n",
        "#@markdown we save the activation and gradients at the\n",
        "#@markdown layer before the attention pool, i.e., layer4.\n",
        "\n",
        "class Hook:\n",
        "    \"\"\"Attaches to a module and records its activations and gradients.\"\"\"\n",
        "\n",
        "    def __init__(self, module: nn.Module):\n",
        "        self.data = None\n",
        "        self.hook = module.register_forward_hook(self.save_grad)\n",
        "\n",
        "    def save_grad(self, module, input, output):\n",
        "        self.data = output\n",
        "        output.requires_grad_(True)\n",
        "        output.retain_grad()\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
        "        self.hook.remove()\n",
        "\n",
        "    @property\n",
        "    def activation(self) -> torch.Tensor:\n",
        "        return self.data\n",
        "\n",
        "    @property\n",
        "    def gradient(self) -> torch.Tensor:\n",
        "        return self.data.grad\n",
        "\n",
        "\n",
        "# Reference: https://arxiv.org/abs/1610.02391\n",
        "def gradCAM(\n",
        "    model: nn.Module,\n",
        "    input: torch.Tensor,\n",
        "    target: torch.Tensor,\n",
        "    layer: nn.Module\n",
        ") -> torch.Tensor:\n",
        "    # Zero out any gradients at the input.\n",
        "    if input.grad is not None:\n",
        "        input.grad.data.zero_()\n",
        "\n",
        "    # Disable gradient settings.\n",
        "    requires_grad = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        requires_grad[name] = param.requires_grad\n",
        "        param.requires_grad_(False)\n",
        "\n",
        "    # Attach a hook to the model at the desired layer.\n",
        "    assert isinstance(layer, nn.Module)\n",
        "    with Hook(layer) as hook:\n",
        "        # Do a forward and backward pass.\n",
        "        output = model(input)\n",
        "        output.backward(target)\n",
        "\n",
        "        grad = hook.gradient.float()\n",
        "        act = hook.activation.float()\n",
        "\n",
        "        # Global average pool gradient across spatial dimension\n",
        "        # to obtain importance weights.\n",
        "        alpha = grad.mean(dim=(2, 3), keepdim=True)\n",
        "        # Weighted combination of activation maps over channel\n",
        "        # dimension.\n",
        "        gradcam = torch.sum(act * alpha, dim=1, keepdim=True)\n",
        "        # We only want neurons with positive influence so we\n",
        "        # clamp any negative ones.\n",
        "        gradcam = torch.clamp(gradcam, min=0)\n",
        "\n",
        "    # Resize gradcam to input resolution.\n",
        "    gradcam = F.interpolate(\n",
        "        gradcam,\n",
        "        input.shape[2:],\n",
        "        mode='bicubic',\n",
        "        align_corners=False)\n",
        "\n",
        "    # Restore gradient settings.\n",
        "    for name, param in model.named_parameters():\n",
        "        param.requires_grad_(requires_grad[name])\n",
        "\n",
        "    return gradcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "_bSzlC60WVkQ",
        "outputId": "a01807fd-4c42-4cb7-c3ef-c97182c992dc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'int' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-61749157d17e>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mattn_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mviz_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_ori\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-abc9ab81191e>\u001b[0m in \u001b[0;36mviz_attn\u001b[0;34m(ori, img, attn_map, blur)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mviz_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mori\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblur\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Ensure the attention map is resized to match the original image dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mattn_map_resized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mori\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mori\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Visualize the original image and attention map overlay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "#@title Run\n",
        "\n",
        "#@markdown #### Image & Caption settings\n",
        "image_path = '000030.jpg' #@param {type:\"string\"}\n",
        "\n",
        "image_caption = 'road signs' #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown #### CLIP model settings\n",
        "clip_model = \"RN50\" #@param [\"RN50\", \"RN101\", \"RN50x4\", \"RN50x16\"]\n",
        "saliency_layer = \"layer4\" #@param [\"layer4\", \"layer3\", \"layer2\", \"layer1\"]\n",
        "#@markdown ---\n",
        "#@markdown #### Visualization settings\n",
        "blur = False #@param {type:\"boolean\"}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(clip_model, device=device, jit=False)\n",
        "\n",
        "# Download the image from the web.\n",
        "# image_path = 'demo.jpg'\n",
        "# urllib.request.urlretrieve(image_url, image_path)\n",
        "\n",
        "image_ori = np.array(Image.open(image_path))\n",
        "image_input = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "image_np = load_image(image_path, model.visual.input_resolution)\n",
        "text_input = clip.tokenize([image_caption]).to(device)\n",
        "\n",
        "attn_map = gradCAM(\n",
        "    model.visual,\n",
        "    image_input,\n",
        "    model.encode_text(text_input).float(),\n",
        "    getattr(model.visual, saliency_layer)\n",
        ")\n",
        "attn_map = attn_map.squeeze().detach().cpu().numpy()\n",
        "\n",
        "viz_attn(image_ori, image_np, attn_map, blur)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_np.shape"
      ],
      "metadata": {
        "id": "gqmzjVxdcQC3",
        "outputId": "f4ba6afa-4ceb-4613-fde7-d0c56af73051",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_ori.shape"
      ],
      "metadata": {
        "id": "SqniYkoJe5Sk",
        "outputId": "a4ac496f-4e11-4636-c802-b125ab45e45d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1028, 1912, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T-WMnKYLe81n"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CLIP GradCAM Visualization ",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}